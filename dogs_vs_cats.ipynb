{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuW-xg_bTsaF"
      },
      "source": [
        "# Dogs vs Cats - Kaggle competition\n",
        "https://www.kaggle.com/competitions/dogs-vs-cats/overview\n",
        "\n",
        "\n",
        "1. **Problem definition**\n",
        "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.\n",
        "\n",
        "2. **Data**\n",
        "Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords. <br>\n",
        "Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:<br>\n",
        "Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. \n",
        "\n",
        "    **The training archive contains 25,000 images of dogs and cats. <br>\n",
        "    Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat).**\n",
        "3. **Evaluation**<br>\n",
        "Performance is evaluated on the percentage of correctly labeled images. (**Accuracy**)\n",
        "4. **Submission**\n",
        "Your submission should have a header. For each image in the test set, predict a label for its id (1 = dog, 0 = cat):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dn-6c02VmqiN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras import layers, Model\n",
        "from shutil import copyfile\n",
        "import datetime\n",
        "\n",
        "# import the InceptionV3 network (not used)\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working with Tensorflow version 2.8.0**"
      ],
      "metadata": {
        "id": "xON0LxftwS3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "eTPac1Ojv412"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sd9dQWa23aj",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# LARGE dataset\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organize folders/sub-folders"
      ],
      "metadata": {
        "id": "IL7vm8vYwfQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DM851ZmN28J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ac8b2e-5f6b-4ae7-e549-69fd8726b546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 12501 images of dogs.\n",
            "There are 12501 images of cats.\n"
          ]
        }
      ],
      "source": [
        "source_path = '/tmp/PetImages'\n",
        "source_path_dogs = os.path.join(source_path, 'Dog')\n",
        "source_path_cats = os.path.join(source_path, 'Cat')\n",
        "\n",
        "print(f\"There are {len(os.listdir(source_path_dogs))} images of dogs.\")\n",
        "print(f\"There are {len(os.listdir(source_path_cats))} images of cats.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "code",
        "id": "F-QkLjxpmyK2"
      },
      "outputs": [],
      "source": [
        "root_dir = '/tmp/cats-v-dogs'\n",
        "train_dir = os.path.join(root_dir, 'training')\n",
        "test_dir = os.path.join(root_dir, 'testing')\n",
        "\n",
        "# create primary directories to use with ImageDataGenerator\n",
        "os.makedirs(root_dir)\n",
        "os.makedirs(train_dir)\n",
        "os.makedirs(test_dir)\n",
        "\n",
        "# create subdirectories for train/test\n",
        "os.makedirs(os.path.join(train_dir, 'cats'))\n",
        "os.makedirs(os.path.join(train_dir, 'dogs'))\n",
        "os.makedirs(os.path.join(test_dir, 'cats'))\n",
        "os.makedirs(os.path.join(test_dir, 'dogs'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define directory variables\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "TESTING_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"cats/\")\n",
        "TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"cats/\")\n",
        "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"dogs/\")\n",
        "TESTING_DOGS_DIR = os.path.join(TESTING_DIR, \"dogs/\")"
      ],
      "metadata": {
        "id": "YKhuS32PxLfL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "code",
        "id": "zvSODo0f9LaU"
      },
      "outputs": [],
      "source": [
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "  # get all images - either dogs or cats\n",
        "  all_imgs = os.listdir(SOURCE)\n",
        "  all_imgs_good = []\n",
        "\n",
        "  # filter empty images = size 0 files\n",
        "  for img in all_imgs:\n",
        "    if os.path.getsize(os.path.join(SOURCE, img)) != 0:\n",
        "      all_imgs_good.append(img)\n",
        "    else:\n",
        "      print(f\"{img} is zero length, so ignoring.\")\n",
        "\n",
        "  # shuffle randomly all the non-empty images\n",
        "  all_imgs_good = random.sample(all_imgs_good, len(all_imgs_good)) \n",
        "\n",
        "  # first part train, last part test\n",
        "  train_list = all_imgs_good[0:int(SPLIT_SIZE * len(all_imgs_good))]\n",
        "  test_list = all_imgs_good[int(SPLIT_SIZE * len(all_imgs_good)):]\n",
        "\n",
        "  # save each image in the proper folder\n",
        "  for img in train_list:\n",
        "    copyfile(os.path.join(SOURCE, img), os.path.join(TRAINING, img))\n",
        "  for img in test_list:\n",
        "    copyfile(os.path.join(SOURCE, img), os.path.join(TESTING, img))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlIdoUeX9S-9"
      },
      "outputs": [],
      "source": [
        "# Define proportion of images used for training\n",
        "split_size = .9\n",
        "\n",
        "# Run the function\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "print(f\"\\n\\nThere are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
        "print(f\"There are {len(os.listdir(TESTING_CATS_DIR))} images of cats for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_DOGS_DIR))} images of dogs for testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create ImageDataGenerators"
      ],
      "metadata": {
        "id": "DjlSrXUWyxLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "code",
        "id": "fQrZfVgz4j2g"
      },
      "outputs": [],
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  # use the ImageDataGenerator to stream the images to model\n",
        "  # we can also use this to further augment the data to decrease overfitting\n",
        "  # the values are typical values found in github projects\n",
        "\n",
        "\n",
        "  # ==== TRAIN ==== #\n",
        "  train_datagen = ImageDataGenerator(rescale=1.0 / 255.,\n",
        "                                     rotation_range=40,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "  \n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=20,\n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  \n",
        "  # ==== TEST ==== #\n",
        "  # no augmentation\n",
        "  validation_datagen = ImageDataGenerator(rescale=1.0 / 255.)\n",
        "\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                            batch_size=20,\n",
        "                                                            class_mode='binary',\n",
        "                                                            target_size=(150, 150))\n",
        "  return train_generator, validation_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM7FxrjGiobD"
      },
      "outputs": [],
      "source": [
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "KrsEwdrfy02e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Transfer Learning - using a pre-trained Inception V3 network\n",
        "# # Download the weights\n",
        "# !wget --no-check-certificate \\\n",
        "#     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "#     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "\n",
        "# # Create an instance of the inception model from the local pre-trained weights\n",
        "# local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'   \n",
        "\n",
        "# # initialize with no weight and then set the weights to the downloaded ones\n",
        "# pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "#                                   include_top = False, \n",
        "#                                   weights = None) \n",
        "# pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# # make all the layers in the pre-trained model non-trainable\n",
        "# for layer in pre_trained_model.layers:\n",
        "#   layer.trainable = False\n",
        "\n",
        "# # extract only a part of the network\n",
        "# last_desired_layer = pre_trained_model.get_layer('mixed7')\n",
        "# last_output = last_desired_layer.output\n",
        "\n",
        "# # create the big model using the functional API\n",
        "# x = layers.Flatten()(last_output)\n",
        "# x = layers.Dense(1024, activation='relu')(x)\n",
        "# x = layers.Dropout(0.2)(x)  \n",
        "# x = layers.Dense(1, activation='sigmoid')(x)        \n",
        "# model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
        "#               loss = 'binary_crossentropy',\n",
        "#               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "XechJ-Qg6GMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([  \n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "oDPK8tUB_O9e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=0.0005), # or Adam, but its a bit slower\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']) "
      ],
      "metadata": {
        "id": "asljAGhAy3w-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "iCneRMCY1h8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping function to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                  patience=3) #stops improving for 3 epochs\n",
        "\n",
        "\n",
        "# Tensorboard - for vizualization\n",
        "def create_tensorboard_callback():\n",
        "  # Create a log directory for storing TensorBoard logs\n",
        "  logdir = os.path.join(\"/tmp/logs\",\n",
        "                        # Make it so the logs get tracked whenever we run an experiment\n",
        "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  return tf.keras.callbacks.TensorBoard(logdir)\n",
        "\n",
        "tensorboard = create_tensorboard_callback()"
      ],
      "metadata": {
        "id": "SMFNJZmTCZv6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "PAdG5pgd1kyC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qE1G6JB4fMn"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping, tensorboard])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vizualize"
      ],
      "metadata": {
        "id": "D06M0C9g3WLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "oC4G8NTXCWNJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /tmp/logs"
      ],
      "metadata": {
        "id": "XxxmdJJd3a24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWZrJN4-65RC"
      },
      "outputs": [],
      "source": [
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(6,6))\n",
        "plt.title('Training and validation accuracy')\n",
        "ax[0].plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "ax[0].plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "\n",
        "ax[1].plot(epochs, loss, 'r', \"Training Loss\")\n",
        "ax[1].plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-U9s5uJFCd8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "dogs_vs_cats.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}